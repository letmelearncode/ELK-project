Index Settings and Mappings (part 2)

Step 1) add a new document with a field (address) which is not part of the structure
PUT /customers/online/124
{
  "name": "Mary Cranford",
  "address": "310 Clark Ave",
  "gender": "female",
  "age": 34,
  "total_spent": 550.75,
  "is_new": false
}

{
  "_index": "customers",
  "_type": "online",
  "_id": "124",
  "_version": 1,
  "result": "created",
  "_shards": {
    "total": 2,
    "successful": 1,
    "failed": 0
  },
  "_seq_no": 0,
  "_primary_term": 1
}

Elasticsearch does not get in our way, it figures out what the appropriate data type should be and indexes it properly.
GET /customers

{
  "customers": {
    "aliases": {},
    "mappings": {
      "online": {
        "properties": {
          "address": {
            "type": "text",
            "fields": {
              "keyword": {
                "type": "keyword",
                "ignore_above": 256
              }
            }
          },
....... etc

Step 2) To enforce Elasticsearch to be strict in only accepting documents matching the current structure:
Set the "dynamic" setting to either:
- "false": the additional fields will be ignored
- "strict": the additional fields will trigger an error

PUT /customers/_mappings/online
{
  "dynamic": "false"
}

{
  "acknowledged": true
}

PUT /customers/_mappings/online
{
  "dynamic": "strict"
}

{
  "acknowledged": true
}

Step 3) this is an example of the error thrown when "dynamic" is set to "strict" and a new field, "retired", is added:
PUT /customers/online/124
{
  "name": "Mary Cranford",
  "address": "310 Clark Ave",
  "gender": "female",
  "age": 34,
  "total_spent": 550.75,
  "is_new": false,
  "retired": true
}

{
  "error": {
    "root_cause": [
      {
        "type": "strict_dynamic_mapping_exception",
        "reason": "mapping set to strict, dynamic introduction of [retired] within [online] is not allowed"
      }
    ],
    "type": "strict_dynamic_mapping_exception",
    "reason": "mapping set to strict, dynamic introduction of [retired] within [online] is not allowed"
  },
  "status": 400
}

Step 4) Test the analyzer to see how they work.
Reference: https://www.elastic.co/guide/en/elasticsearch/reference/current/_testing_analyzers.html

POST _analyze
{
  "analyzer": "whitespace",
  "text":     "The quick brown fox."
}

{
  "tokens": [
    {
      "token": "The",
      "start_offset": 0,
      "end_offset": 3,
      "type": "word",
      "position": 0
    },
    {
      "token": "quick",
      "start_offset": 4,
      "end_offset": 9,
      "type": "word",
      "position": 1
    },
    {
      "token": "brown",
      "start_offset": 10,
      "end_offset": 15,
      "type": "word",
      "position": 2
    },
    {
      "token": "fox.",
      "start_offset": 16,
      "end_offset": 20,
      "type": "word",
      "position": 3
    }
  ]
}

Step 5) The standard Analyzer removes punctuation and lowercases all words.
POST _analyze
{
  "analyzer": "standard",
  "text":     "The quick brown fox."
}

{
  "tokens": [
    {
      "token": "the",
      "start_offset": 0,
      "end_offset": 3,
      "type": "<ALPHANUM>",
      "position": 0
    },
    {
      "token": "quick",
      "start_offset": 4,
      "end_offset": 9,
      "type": "<ALPHANUM>",
      "position": 1
    },
    {
      "token": "brown",
      "start_offset": 10,
      "end_offset": 15,
      "type": "<ALPHANUM>",
      "position": 2
    },
    {
      "token": "fox",
      "start_offset": 16,
      "end_offset": 19,
      "type": "<ALPHANUM>",
      "position": 3
    }
  ]
}

Step 6) The simple analyzer works like the standard analyzer but also breaks on numbers (anything that is not a letter):
POST _analyze
{
  "analyzer": "simple",
  "text":     "The quick123 brown456 fox."
}

{
  "tokens": [
    {
      "token": "the",
      "start_offset": 0,
      "end_offset": 3,
      "type": "word",
      "position": 0
    },
    {
      "token": "quick",
      "start_offset": 4,
      "end_offset": 9,
      "type": "word",
      "position": 1
    },
    {
      "token": "brown",
      "start_offset": 13,
      "end_offset": 18,
      "type": "word",
      "position": 2
    },
    {
      "token": "fox",
      "start_offset": 22,
      "end_offset": 25,
      "type": "word",
      "position": 3
    }
  ]
}

Step 7) the "english" analyzer will try to identify words in English and their roots.
POST _analyze
{
  "analyzer": "english",
  "text":     "The quick:; running brown456 foxes."
}

{
  "tokens": [
    {
      "token": "quick",
      "start_offset": 4,
      "end_offset": 9,
      "type": "<ALPHANUM>",
      "position": 1
    },
    {
      "token": "run",
      "start_offset": 12,
      "end_offset": 19,
      "type": "<ALPHANUM>",
      "position": 2
    },
    {
      "token": "brown456",
      "start_offset": 20,
      "end_offset": 28,
      "type": "<ALPHANUM>",
      "position": 3
    },
    {
      "token": "fox",
      "start_offset": 29,
      "end_offset": 34,
      "type": "<ALPHANUM>",
      "position": 4
    }
  ]
}


Step 8) you can define your own analyzer
Reference: https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-custom-analyzer.html

